{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building POS Tagger using Keras Library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with Twitter *Hindi-English* code mixed tweets. For the purpose of data, we have 1981 tweets which are in **conll** format. They are tagged manually with the *language* and *POS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers import Embedding # new!\n",
    "from keras.layers import Conv1D, SpatialDropout1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint # new! \n",
    "from sklearn.model_selection import train_test_split\n",
    "import os # new! \n",
    "from sklearn.metrics import roc_auc_score, roc_curve # new!\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # new!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to define arr with (word, lang, tag)\n",
    "def make_arr(f1):\n",
    "    twitter_file = open(f1, \"r\")\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for line in twitter_file:\n",
    "        temp = line.split('\\t')\n",
    "        \n",
    "        if temp[0] == '\\n':\n",
    "            sentences.append(sent)\n",
    "            sent = []\n",
    "            continue\n",
    "\n",
    "        check = list(temp[2])\n",
    "        if '\\n' in check:\n",
    "            check.remove('\\n')\n",
    "\n",
    "        temp[2] = ''.join(check)\n",
    "        sent.append((temp[0], temp[1], temp[2]))\n",
    "\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = make_arr(\"Twitter_file.txt\")\n",
    "no_of_sentences = 1981 # number of sentences to take for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1981"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the sentences words and tags into two different arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, sentence_tags = [], []\n",
    "for sent in tagged_sentences:\n",
    "    sentence, lang, tags = zip(*sent)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1981, 1981)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), len(sentence_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['I', 'will', 'read', 'kal', 'pakka', '.', 'Have', 'to', 'study',\n",
       "        'right', 'now', 'okay', '?', '\\xf0\\x9f\\x98\\x80',\n",
       "        'https://t.co/C2SrZhfJfK'], dtype='|S23'),\n",
       " array(['PR_PRP', 'V_VAUX', 'V_VM', 'RB_ALC', 'JJ', 'RD_PUNC', 'V_VM',\n",
       "        'RP_RPD', 'N_NN', 'RB_AMN', 'RB_AMN', 'RP_INJ', 'RD_PUNC', 'E',\n",
       "        'U'], dtype='|S7'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0], sentence_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences[0]), len(sentence_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the required number of sentences *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences[:no_of_sentences]\n",
    "sentence_tags = sentence_tags[:no_of_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_sentences, test_sentences, train_tags, test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I' 'will' 'read' 'kal' 'pakka' '.' 'Have' 'to' 'study' 'right' 'now'\n",
      " 'okay' '?' '\\xf0\\x9f\\x98\\x80' 'https://t.co/C2SrZhfJfK'] ['PR_PRP' 'V_VAUX' 'V_VM' 'RB_ALC' 'JJ' 'RD_PUNC' 'V_VM' 'RP_RPD' 'N_NN'\n",
      " 'RB_AMN' 'RB_AMN' 'RP_INJ' 'RD_PUNC' 'E' 'U']\n"
     ]
    }
   ],
   "source": [
    "print sentences[0], sentence_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the embedding layer to work, find the count of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "\n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    "        \n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@' 'PR_PRP' 'V_VM' 'RB_ALC' 'N_NN' 'DM_DMD' 'JJ' 'N_NNP' 'PSP' 'N_NST'\n",
      " 'PSP' 'N_NN' 'V_VAUX' 'V_VM' 'DM_DMD' 'JJ' 'N_NN' 'V_VM' 'PR_PRP'\n",
      " 'RP_INTF' 'JJ' 'RP_NEG' 'V_VAUX' 'RB_ALC' 'E']\n"
     ]
    }
   ],
   "source": [
    "print train_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the words to the numpy array so that, numerical data can be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4353, 267, 975, 3527, 5785, 3400, 2567, 1464, 5790, 3938, 5108, 1595, 7059, 2561, 3400, 4586, 2567, 400, 1628, 8501, 7035, 2394, 7333, 5545, 2904]\n",
      "[18, 39, 3, 35, 17, 33, 5, 10, 36, 15, 36, 17, 29, 3, 33, 5, 17, 3, 39, 13, 5, 7, 29, 35, 20]\n",
      "[7704, 8219, 1, 6622, 4811, 1, 3754, 6551, 1, 7638, 3953, 7059, 400, 2490, 1, 1044, 3754, 14]\n",
      "[18, 18, 18, 4, 3, 10, 32, 17, 3, 17, 3, 29, 29, 39, 17, 10, 32, 18]\n",
      "25 25\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "\n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    "\n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])\n",
    "\n",
    "print(train_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_sentences_X[0])\n",
    "print(test_tags_y[0])\n",
    "print len(train_sentences_X[0]), len((train_tags_y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad the sequences because Keras can only work with fixed size sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4353  267  975 3527 5785 3400 2567 1464 5790 3938 5108 1595 7059 2561\n",
      " 3400 4586 2567  400 1628 8501 7035 2394 7333 5545 2904    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[7704 8219    1 6622 4811    1 3754 6551    1 7638 3953 7059  400 2490\n",
      "    1 1044 3754   14    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[18 39  3 35 17 33  5 10 36 15 36 17 29  3 33  5 17  3 39 13  5  7 29 35\n",
      " 20  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[18 18 18  4  3 10 32 17  3 17  3 29 29 39 17 10 32 18  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define our neural architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 207, 128)          1115776   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 207, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 207, 40)           20520     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 207, 40)           0         \n",
      "=================================================================\n",
      "Total params: 1,924,776\n",
      "Trainable params: 1,924,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we have 39 tags for each word we need to convert it to ONE HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print cat_train_tags_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(0.001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1267 samples, validate on 317 samples\n",
      "Epoch 1/40\n",
      "1267/1267 [==============================] - 37s 29ms/step - loss: 2.0291 - acc: 0.8070 - val_loss: 0.6290 - val_acc: 0.8959\n",
      "Epoch 2/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.4857 - acc: 0.8976 - val_loss: 0.4161 - val_acc: 0.8941\n",
      "Epoch 3/40\n",
      "1267/1267 [==============================] - 33s 26ms/step - loss: 0.3779 - acc: 0.8987 - val_loss: 0.3477 - val_acc: 0.8959\n",
      "Epoch 4/40\n",
      "1267/1267 [==============================] - 34s 26ms/step - loss: 0.3393 - acc: 0.9029 - val_loss: 0.3403 - val_acc: 0.9048\n",
      "Epoch 5/40\n",
      "1267/1267 [==============================] - 33s 26ms/step - loss: 0.3296 - acc: 0.9087 - val_loss: 0.3382 - val_acc: 0.9058\n",
      "Epoch 6/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.3254 - acc: 0.9109 - val_loss: 0.3317 - val_acc: 0.9115\n",
      "Epoch 7/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.3195 - acc: 0.9124 - val_loss: 0.3399 - val_acc: 0.9088\n",
      "Epoch 8/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.3142 - acc: 0.9126 - val_loss: 0.3338 - val_acc: 0.9101\n",
      "Epoch 9/40\n",
      "1267/1267 [==============================] - 33s 26ms/step - loss: 0.3094 - acc: 0.9131 - val_loss: 0.3432 - val_acc: 0.9096\n",
      "Epoch 10/40\n",
      "1267/1267 [==============================] - 36s 28ms/step - loss: 0.3054 - acc: 0.9133 - val_loss: 0.3504 - val_acc: 0.9093\n",
      "Epoch 11/40\n",
      "1267/1267 [==============================] - 33s 26ms/step - loss: 0.3022 - acc: 0.9134 - val_loss: 0.3549 - val_acc: 0.9088\n",
      "Epoch 12/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.2997 - acc: 0.9137 - val_loss: 0.3607 - val_acc: 0.9087\n",
      "Epoch 13/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2977 - acc: 0.9140 - val_loss: 0.3686 - val_acc: 0.9085\n",
      "Epoch 14/40\n",
      "1267/1267 [==============================] - 34s 26ms/step - loss: 0.2964 - acc: 0.9141 - val_loss: 0.3695 - val_acc: 0.9085\n",
      "Epoch 15/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.2958 - acc: 0.9141 - val_loss: 0.3740 - val_acc: 0.9082\n",
      "Epoch 16/40\n",
      "1267/1267 [==============================] - 37s 29ms/step - loss: 0.2951 - acc: 0.9143 - val_loss: 0.3682 - val_acc: 0.9086\n",
      "Epoch 17/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2945 - acc: 0.9146 - val_loss: 0.3710 - val_acc: 0.9089\n",
      "Epoch 18/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2937 - acc: 0.9146 - val_loss: 0.3684 - val_acc: 0.9091\n",
      "Epoch 19/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2926 - acc: 0.9148 - val_loss: 0.3684 - val_acc: 0.9092\n",
      "Epoch 20/40\n",
      "1267/1267 [==============================] - 33s 26ms/step - loss: 0.2913 - acc: 0.9151 - val_loss: 0.3656 - val_acc: 0.9096\n",
      "Epoch 21/40\n",
      "1267/1267 [==============================] - 33s 26ms/step - loss: 0.2897 - acc: 0.9156 - val_loss: 0.3649 - val_acc: 0.9100\n",
      "Epoch 22/40\n",
      "1267/1267 [==============================] - 36s 28ms/step - loss: 0.2880 - acc: 0.9168 - val_loss: 0.3643 - val_acc: 0.9096\n",
      "Epoch 23/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2862 - acc: 0.9169 - val_loss: 0.3670 - val_acc: 0.9109\n",
      "Epoch 24/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2845 - acc: 0.9175 - val_loss: 0.3629 - val_acc: 0.9115\n",
      "Epoch 25/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.2820 - acc: 0.9188 - val_loss: 0.3546 - val_acc: 0.9133\n",
      "Epoch 26/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2790 - acc: 0.9232 - val_loss: 0.3505 - val_acc: 0.9143\n",
      "Epoch 27/40\n",
      "1267/1267 [==============================] - 33s 26ms/step - loss: 0.2750 - acc: 0.9251 - val_loss: 0.3374 - val_acc: 0.9185\n",
      "Epoch 28/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2819 - acc: 0.9254 - val_loss: 0.3189 - val_acc: 0.9252\n",
      "Epoch 29/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2707 - acc: 0.9330 - val_loss: 0.3084 - val_acc: 0.9235\n",
      "Epoch 30/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2616 - acc: 0.9333 - val_loss: 0.2942 - val_acc: 0.9283\n",
      "Epoch 31/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2531 - acc: 0.9349 - val_loss: 0.2875 - val_acc: 0.9294\n",
      "Epoch 32/40\n",
      "1267/1267 [==============================] - 34s 26ms/step - loss: 0.2443 - acc: 0.9374 - val_loss: 0.2811 - val_acc: 0.9319\n",
      "Epoch 33/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2357 - acc: 0.9389 - val_loss: 0.2787 - val_acc: 0.9330\n",
      "Epoch 34/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2268 - acc: 0.9419 - val_loss: 0.2731 - val_acc: 0.9336\n",
      "Epoch 35/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2177 - acc: 0.9449 - val_loss: 0.2687 - val_acc: 0.9354\n",
      "Epoch 36/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2084 - acc: 0.9469 - val_loss: 0.2662 - val_acc: 0.9379\n",
      "Epoch 37/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.1988 - acc: 0.9502 - val_loss: 0.2587 - val_acc: 0.9386\n",
      "Epoch 38/40\n",
      "1267/1267 [==============================] - 33s 26ms/step - loss: 0.1886 - acc: 0.9522 - val_loss: 0.2496 - val_acc: 0.9418\n",
      "Epoch 39/40\n",
      "1267/1267 [==============================] - 33s 26ms/step - loss: 0.1780 - acc: 0.9555 - val_loss: 0.2346 - val_acc: 0.9438\n",
      "Epoch 40/40\n",
      "1267/1267 [==============================] - 34s 26ms/step - loss: 0.1685 - acc: 0.9585 - val_loss: 0.2194 - val_acc: 0.9475\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1dd15fa190>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397/397 [==============================] - 3s 8ms/step\n",
      "(set(['acc']), set([95.0194091430539]))\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print ({model.metrics_names[1]},  {scores[1] * 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert back the categorical to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "207\n",
      "397 397\n",
      "18\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_sentences_X)\n",
    "\n",
    "print len(predictions[0])\n",
    "print len(test_tags_y[0])\n",
    "\n",
    "# print first few predictions\n",
    "token_sequences = logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})\n",
    "# print(logits_to_tokens(predictions[:2], {i: t for t, i in tag2index.items()}))\n",
    "test_tags_sequences = logits_to_tokens(test_tags_y, {i: t for t, i in tag2index.items()})\n",
    "\n",
    "print len(predictions), len(test_tags_sequences)\n",
    "# token_sequences[0] = filter(lambda a: a != '-PAD-', token_sequences[0])\n",
    "print len(token_sequences[0][:len(test_tags[0])])\n",
    "print len(test_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
