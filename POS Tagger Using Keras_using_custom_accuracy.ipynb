{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building POS Tagger using Keras Library without considering PAD values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with Twitter *Hindi-English* code mixed tweets. For the purpose of data, we have 1981 tweets which are in **conll** format. They are tagged manually with the *language* and *POS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers import Embedding # new!\n",
    "from keras.layers import Conv1D, SpatialDropout1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint # new! \n",
    "from sklearn.model_selection import train_test_split\n",
    "import os # new! \n",
    "from sklearn.metrics import roc_auc_score, roc_curve # new!\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # new!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    " \n",
    "def ignore_class_accuracy(to_ignore=0):\n",
    "    def ignore_accuracy(y_true, y_pred):\n",
    "        y_true_class = K.argmax(y_true, axis=-1)\n",
    "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
    " \n",
    "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
    "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
    "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
    "        return accuracy\n",
    "    return ignore_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to define arr with (word, lang, tag)\n",
    "def make_arr(f1):\n",
    "    twitter_file = open(f1, \"r\")\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for line in twitter_file:\n",
    "        temp = line.split('\\t')\n",
    "        \n",
    "        if temp[0] == '\\n':\n",
    "            sentences.append(sent)\n",
    "            sent = []\n",
    "            continue\n",
    "\n",
    "        check = list(temp[2])\n",
    "        if '\\n' in check:\n",
    "            check.remove('\\n')\n",
    "\n",
    "        temp[2] = ''.join(check)\n",
    "        sent.append((temp[0], temp[1], temp[2]))\n",
    "\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = make_arr(\"Twitter_file.txt\")\n",
    "no_of_sentences = 1981 # number of sentences to take for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1981"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the sentences words and tags into two different arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, sentence_tags = [], []\n",
    "for sent in tagged_sentences:\n",
    "    sentence, lang, tags = zip(*sent)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1981, 1981)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), len(sentence_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['I', 'will', 'read', 'kal', 'pakka', '.', 'Have', 'to', 'study',\n",
       "        'right', 'now', 'okay', '?', '\\xf0\\x9f\\x98\\x80',\n",
       "        'https://t.co/C2SrZhfJfK'], dtype='|S23'),\n",
       " array(['PR_PRP', 'V_VAUX', 'V_VM', 'RB_ALC', 'JJ', 'RD_PUNC', 'V_VM',\n",
       "        'RP_RPD', 'N_NN', 'RB_AMN', 'RB_AMN', 'RP_INJ', 'RD_PUNC', 'E',\n",
       "        'U'], dtype='|S7'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0], sentence_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences[0]), len(sentence_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the required number of sentences *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences[:no_of_sentences]\n",
    "sentence_tags = sentence_tags[:no_of_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_sentences, test_sentences, train_tags, test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I' 'will' 'read' 'kal' 'pakka' '.' 'Have' 'to' 'study' 'right' 'now'\n",
      " 'okay' '?' '\\xf0\\x9f\\x98\\x80' 'https://t.co/C2SrZhfJfK'] ['PR_PRP' 'V_VAUX' 'V_VM' 'RB_ALC' 'JJ' 'RD_PUNC' 'V_VM' 'RP_RPD' 'N_NN'\n",
      " 'RB_AMN' 'RB_AMN' 'RP_INJ' 'RD_PUNC' 'E' 'U']\n"
     ]
    }
   ],
   "source": [
    "print sentences[0], sentence_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the embedding layer to work, find the count of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "\n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    "        \n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@' 'PSP' 'RD_PUNC' 'PR_PRP' 'PSP' 'V_VM' 'V_VM' 'DT' 'V_VM' 'N_NN'\n",
      " 'RD_PUNC' 'JJ' 'JJ' 'RD_PUNC' 'JJ' 'JJ' 'N_NN' 'V_VAUX' 'V_VM' 'N_NN'\n",
      " 'RD_PUNC' 'E']\n"
     ]
    }
   ],
   "source": [
    "print train_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the words to the numpy array so that, numerical data can be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6552, 667, 3708, 3233, 7988, 3152, 1544, 6669, 6879, 2583, 1068, 5675, 5675, 3758, 4575, 4575, 2574, 3864, 3286, 7729, 3758, 4427]\n",
      "[18, 36, 32, 39, 36, 4, 4, 7, 4, 17, 32, 6, 6, 32, 6, 6, 17, 29, 4, 17, 32, 20]\n",
      "[1, 7367, 2324, 3574, 673, 4370, 7015, 1038, 1850, 4727, 4727, 50, 1, 1966, 3233, 1242, 3372, 1, 6581, 8355, 343, 1294]\n",
      "[18, 21, 16, 15, 36, 21, 17, 17, 17, 6, 6, 36, 20, 16, 39, 4, 7, 17, 4, 35, 32, 20]\n",
      "22 22\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "\n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    "\n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])\n",
    "\n",
    "print(train_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_sentences_X[0])\n",
    "print(test_tags_y[0])\n",
    "print len(train_sentences_X[0]), len((train_tags_y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad the sequences because Keras can only work with fixed size sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6552  667 3708 3233 7988 3152 1544 6669 6879 2583 1068 5675 5675 3758\n",
      " 4575 4575 2574 3864 3286 7729 3758 4427    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[   1 7367 2324 3574  673 4370 7015 1038 1850 4727 4727   50    1 1966\n",
      " 3233 1242 3372    1 6581 8355  343 1294    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0]\n",
      "[18 36 32 39 36  4  4  7  4 17 32  6  6 32  6  6 17 29  4 17 32 20  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[18 21 16 15 36 21 17 17 17  6  6 36 20 16 39  4  7 17  4 35 32 20  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define our neural architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(0.001),\n",
    "              metrics=['accuracy', ignore_class_accuracy(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 207, 128)          1116544   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 207, 512)          788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 207, 40)           20520     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 207, 40)           0         \n",
      "=================================================================\n",
      "Total params: 1,925,544\n",
      "Trainable params: 1,925,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we have 39 tags for each word we need to convert it to ONE HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print cat_train_tags_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1267 samples, validate on 317 samples\n",
      "Epoch 1/40\n",
      "1267/1267 [==============================] - 38s 30ms/step - loss: 1.9498 - acc: 0.8968 - ignore_accuracy: 0.0000e+00 - val_loss: 0.6445 - val_acc: 0.8982 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 2/40\n",
      "1267/1267 [==============================] - 38s 30ms/step - loss: 0.4853 - acc: 0.8977 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3878 - val_acc: 0.8976 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 3/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.3636 - acc: 0.9044 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3377 - val_acc: 0.9117 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 4/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.3369 - acc: 0.9077 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3308 - val_acc: 0.9018 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 5/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.3306 - acc: 0.9024 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3252 - val_acc: 0.9042 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 6/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.3243 - acc: 0.9080 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3198 - val_acc: 0.9129 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 7/40\n",
      "1267/1267 [==============================] - 36s 28ms/step - loss: 0.3188 - acc: 0.9121 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3191 - val_acc: 0.9143 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 8/40\n",
      "1267/1267 [==============================] - 36s 28ms/step - loss: 0.3138 - acc: 0.9128 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3200 - val_acc: 0.9141 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 9/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.3111 - acc: 0.9130 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3220 - val_acc: 0.9137 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 10/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.3144 - acc: 0.9132 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3231 - val_acc: 0.9133 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 11/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.3110 - acc: 0.9132 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3220 - val_acc: 0.9133 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 12/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.3080 - acc: 0.9137 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3264 - val_acc: 0.9128 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 13/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.3049 - acc: 0.9139 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3271 - val_acc: 0.9129 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 14/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.3020 - acc: 0.9140 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3256 - val_acc: 0.9130 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 15/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.2996 - acc: 0.9145 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3250 - val_acc: 0.9130 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 16/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2984 - acc: 0.9147 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3170 - val_acc: 0.9140 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 17/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.2969 - acc: 0.9149 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3171 - val_acc: 0.9147 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 18/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.2948 - acc: 0.9173 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3172 - val_acc: 0.9144 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 19/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.2938 - acc: 0.9150 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3152 - val_acc: 0.9156 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 20/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2915 - acc: 0.9190 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3139 - val_acc: 0.9147 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 21/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2888 - acc: 0.9170 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3098 - val_acc: 0.9175 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 22/40\n",
      "1267/1267 [==============================] - 36s 28ms/step - loss: 0.2877 - acc: 0.9201 - ignore_accuracy: 0.0000e+00 - val_loss: 0.3055 - val_acc: 0.9178 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 23/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.2832 - acc: 0.9215 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2996 - val_acc: 0.9219 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 24/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.2791 - acc: 0.9253 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2949 - val_acc: 0.9222 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 25/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.2739 - acc: 0.9265 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2884 - val_acc: 0.9258 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 26/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2679 - acc: 0.9289 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2858 - val_acc: 0.9256 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 27/40\n",
      "1267/1267 [==============================] - 38s 30ms/step - loss: 0.2704 - acc: 0.9285 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2928 - val_acc: 0.9233 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 28/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2677 - acc: 0.9305 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2700 - val_acc: 0.9302 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 29/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.2567 - acc: 0.9337 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2590 - val_acc: 0.9334 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 30/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.2486 - acc: 0.9364 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2512 - val_acc: 0.9377 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 31/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.2409 - acc: 0.9412 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2457 - val_acc: 0.9386 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 32/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.2330 - acc: 0.9424 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2397 - val_acc: 0.9404 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 33/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.2246 - acc: 0.9451 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2316 - val_acc: 0.9420 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 34/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.2152 - acc: 0.9475 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2232 - val_acc: 0.9438 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 35/40\n",
      "1267/1267 [==============================] - 34s 27ms/step - loss: 0.2049 - acc: 0.9500 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2145 - val_acc: 0.9463 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 36/40\n",
      "1267/1267 [==============================] - 37s 29ms/step - loss: 0.1939 - acc: 0.9532 - ignore_accuracy: 0.0000e+00 - val_loss: 0.2048 - val_acc: 0.9487 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 37/40\n",
      "1267/1267 [==============================] - 35s 28ms/step - loss: 0.1820 - acc: 0.9550 - ignore_accuracy: 0.0000e+00 - val_loss: 0.1960 - val_acc: 0.9508 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 38/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.1700 - acc: 0.9576 - ignore_accuracy: 0.0000e+00 - val_loss: 0.1852 - val_acc: 0.9537 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 39/40\n",
      "1267/1267 [==============================] - 36s 29ms/step - loss: 0.1577 - acc: 0.9613 - ignore_accuracy: 0.0000e+00 - val_loss: 0.1766 - val_acc: 0.9566 - val_ignore_accuracy: 0.0000e+00\n",
      "Epoch 40/40\n",
      "1267/1267 [==============================] - 35s 27ms/step - loss: 0.1455 - acc: 0.9658 - ignore_accuracy: 0.0000e+00 - val_loss: 0.1682 - val_acc: 0.9612 - val_ignore_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdf4e36bd90>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397/397 [==============================] - 3s 8ms/step\n",
      "(set(['acc']), set([96.08780842283811]))\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print ({model.metrics_names[1]},  {scores[1] * 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert back the categorical to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n",
      "207\n",
      "397 397\n",
      "22\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_sentences_X)\n",
    "\n",
    "print len(predictions[0])\n",
    "print len(test_tags_y[0])\n",
    "\n",
    "# print first few predictions\n",
    "token_sequences = logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})\n",
    "# print(logits_to_tokens(predictions[:2], {i: t for t, i in tag2index.items()}))\n",
    "test_tags_sequences = logits_to_tokens(test_tags_y, {i: t for t, i in tag2index.items()})\n",
    "\n",
    "print len(predictions), len(test_tags_sequences)\n",
    "# token_sequences[0] = filter(lambda a: a != '-PAD-', token_sequences[0])\n",
    "# print (test_sentences[0])\n",
    "print len(token_sequences[0][:len(test_tags[0])])\n",
    "print len(test_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
