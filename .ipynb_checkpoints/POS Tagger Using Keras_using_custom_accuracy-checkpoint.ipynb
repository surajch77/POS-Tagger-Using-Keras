{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building POS Tagger using Keras Library "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to work with Twitter *Hindi-English* code mixed tweets. For the purpose of data, we have 1981 tweets which are in **conll** format. They are tagged manually with the *language* and *POS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.layers import Embedding # new!\n",
    "from keras.layers import Conv1D, SpatialDropout1D, GlobalMaxPooling1D\n",
    "\n",
    "from keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint # new! \n",
    "from sklearn.model_selection import train_test_split\n",
    "import os # new! \n",
    "from sklearn.metrics import roc_auc_score, roc_curve # new!\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt # new!\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to define arr with (word, lang, tag)\n",
    "def make_arr(f1):\n",
    "    twitter_file = open(f1, \"r\")\n",
    "    sentences = []\n",
    "    sent = []\n",
    "    for line in twitter_file:\n",
    "        temp = line.split('\\t')\n",
    "        \n",
    "        if temp[0] == '\\n':\n",
    "            sentences.append(sent)\n",
    "            sent = []\n",
    "            continue\n",
    "\n",
    "        check = list(temp[2])\n",
    "        if '\\n' in check:\n",
    "            check.remove('\\n')\n",
    "\n",
    "        temp[2] = ''.join(check)\n",
    "        sent.append((temp[0], temp[1], temp[2]))\n",
    "\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_sentences = make_arr(\"Twitter_file.txt\")\n",
    "no_of_sentences = 1981 # number of sentences to take for corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1981"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tagged_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate the sentences words and tags into two different arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences, sentence_tags = [], []\n",
    "for sent in tagged_sentences:\n",
    "    sentence, lang, tags = zip(*sent)\n",
    "    sentences.append(np.array(sentence))\n",
    "    sentence_tags.append(np.array(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1981, 1981)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences), len(sentence_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['I', 'will', 'read', 'kal', 'pakka', '.', 'Have', 'to', 'study',\n",
       "        'right', 'now', 'okay', '?', '\\xf0\\x9f\\x98\\x80',\n",
       "        'https://t.co/C2SrZhfJfK'], dtype='|S23'),\n",
       " array(['PR_PRP', 'V_VAUX', 'V_VM', 'RB_ALC', 'JJ', 'RD_PUNC', 'V_VM',\n",
       "        'RP_RPD', 'N_NN', 'RB_AMN', 'RB_AMN', 'RP_INJ', 'RD_PUNC', 'E',\n",
       "        'U'], dtype='|S7'))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0], sentence_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences[0]), len(sentence_tags[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the required number of sentences *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences[:no_of_sentences]\n",
    "sentence_tags = sentence_tags[:no_of_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_sentences, test_sentences, train_tags, test_tags) = train_test_split(sentences, sentence_tags, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I' 'will' 'read' 'kal' 'pakka' '.' 'Have' 'to' 'study' 'right' 'now'\n",
      " 'okay' '?' '\\xf0\\x9f\\x98\\x80' 'https://t.co/C2SrZhfJfK'] ['PR_PRP' 'V_VAUX' 'V_VM' 'RB_ALC' 'JJ' 'RD_PUNC' 'V_VM' 'RP_RPD' 'N_NN'\n",
      " 'RB_AMN' 'RB_AMN' 'RP_INJ' 'RD_PUNC' 'E' 'U']\n"
     ]
    }
   ],
   "source": [
    "print sentences[0], sentence_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the embedding layer to work, find the count of unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, tags = set([]), set([])\n",
    " \n",
    "for s in train_sentences:\n",
    "    for w in s:\n",
    "        words.add(w.lower())\n",
    "\n",
    "for ts in train_tags:\n",
    "    for t in ts:\n",
    "        tags.add(t)\n",
    "        \n",
    "word2index = {w: i + 2 for i, w in enumerate(list(words))}\n",
    "word2index['-PAD-'] = 0  # The special value used for padding\n",
    "word2index['-OOV-'] = 1  # The special value used for OOVs\n",
    " \n",
    "tag2index = {t: i + 1 for i, t in enumerate(list(tags))}\n",
    "tag2index['-PAD-'] = 0  # The special value used to padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@' '@' '@' 'JJ' 'RD_PUNC' 'PR_PRP' 'RB_AMN' 'V_VM' '$' 'RP_RPD' 'V_VM'\n",
      " 'DT' 'N_NN' 'V_VM' 'PSP' 'N_NN' 'CC' 'N_NN' 'PSP' 'N_NN' 'V_VM' 'RP_RPD'\n",
      " 'RP_NEG']\n"
     ]
    }
   ],
   "source": [
    "print train_tags[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the words to the numpy array so that, numerical data can be used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4484, 3644, 1224, 2679, 350, 2353, 2862, 6276, 6370, 1100, 7621, 2580, 2288, 5722, 4749, 1835, 5705, 8309, 680, 3274, 4534, 6779, 7070]\n",
      "[18, 18, 18, 5, 32, 39, 16, 3, 12, 4, 3, 6, 17, 3, 36, 17, 21, 17, 36, 17, 3, 4, 7]\n",
      "[1, 4377, 2161, 5192, 1, 8030, 1, 6220, 1896, 674, 2642, 5494, 8575, 1029, 6654, 1761, 3370, 7348, 7674, 6640, 7348, 4478, 6640, 1612, 1]\n",
      "[18, 21, 39, 17, 17, 36, 3, 32, 29, 21, 3, 39, 3, 3, 39, 5, 17, 21, 3, 10, 21, 3, 10, 32, 17]\n",
      "23 23\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X, test_sentences_X, train_tags_y, test_tags_y = [], [], [], []\n",
    " \n",
    "for s in train_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    train_sentences_X.append(s_int)\n",
    "\n",
    "for s in test_sentences:\n",
    "    s_int = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            s_int.append(word2index[w.lower()])\n",
    "        except KeyError:\n",
    "            s_int.append(word2index['-OOV-'])\n",
    " \n",
    "    test_sentences_X.append(s_int)\n",
    "\n",
    "for s in train_tags:\n",
    "    train_tags_y.append([tag2index[t] for t in s])\n",
    "\n",
    "for s in test_tags:\n",
    "    test_tags_y.append([tag2index[t] for t in s])\n",
    "\n",
    "print(train_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_sentences_X[0])\n",
    "print(test_tags_y[0])\n",
    "print len(train_sentences_X[0]), len((train_tags_y[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad the sequences because Keras can only work with fixed size sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = len(max(train_sentences_X, key=len))\n",
    "print(MAX_LENGTH)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4484 3644 1224 2679  350 2353 2862 6276 6370 1100 7621 2580 2288 5722\n",
      " 4749 1835 5705 8309  680 3274 4534 6779 7070    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n",
      "[   1 4377 2161 5192    1 8030    1 6220 1896  674 2642 5494 8575 1029\n",
      " 6654 1761 3370 7348 7674 6640 7348 4478 6640 1612    1    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0]\n",
      "[18 18 18  5 32 39 16  3 12  4  3  6 17  3 36 17 21 17 36 17  3  4  7  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[18 21 39 17 17 36  3 32 29 21  3 39  3  3 39  5 17 21  3 10 21  3 10 32\n",
      " 17  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "train_sentences_X = pad_sequences(train_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "test_sentences_X = pad_sequences(test_sentences_X, maxlen=MAX_LENGTH, padding='post')\n",
    "train_tags_y = pad_sequences(train_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    "test_tags_y = pad_sequences(test_tags_y, maxlen=MAX_LENGTH, padding='post')\n",
    " \n",
    "print(train_sentences_X[0])\n",
    "print(test_sentences_X[0])\n",
    "print(train_tags_y[0])\n",
    "print(test_tags_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "print(len(tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define our neural architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(InputLayer(input_shape=(MAX_LENGTH, )))\n",
    "model.add(Embedding(len(word2index), 128))\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(TimeDistributed(Dense(len(tag2index))))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 93, 128)           1117952   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 93, 512)           788480    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 93, 40)            20520     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 93, 40)            0         \n",
      "=================================================================\n",
      "Total params: 1,926,952\n",
      "Trainable params: 1,926,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we have 33 tags for each word we need to convert it to ONE HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(sequences, categories):\n",
    "    cat_sequences = []\n",
    "    for s in sequences:\n",
    "        cats = []\n",
    "        for item in s:\n",
    "            cats.append(np.zeros(categories))\n",
    "            cats[-1][item] = 1.0\n",
    "        cat_sequences.append(cats)\n",
    "    return np.array(cat_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "cat_train_tags_y = to_categorical(train_tags_y, len(tag2index))\n",
    "print cat_train_tags_y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=Adam(0.001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1267 samples, validate on 317 samples\n",
      "Epoch 1/40\n",
      "1267/1267 [==============================] - 19s 15ms/step - loss: 2.3033 - acc: 0.6976 - val_loss: 0.9984 - val_acc: 0.7707\n",
      "Epoch 2/40\n",
      "1267/1267 [==============================] - 15s 12ms/step - loss: 0.8939 - acc: 0.7760 - val_loss: 0.8419 - val_acc: 0.7713\n",
      "Epoch 3/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.7887 - acc: 0.7803 - val_loss: 0.7502 - val_acc: 0.7932\n",
      "Epoch 4/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.7290 - acc: 0.8044 - val_loss: 0.7440 - val_acc: 0.7966\n",
      "Epoch 5/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.7033 - acc: 0.8059 - val_loss: 0.7207 - val_acc: 0.8019\n",
      "Epoch 6/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.6845 - acc: 0.8081 - val_loss: 0.7286 - val_acc: 0.8011\n",
      "Epoch 7/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.6712 - acc: 0.8091 - val_loss: 0.7443 - val_acc: 0.8002\n",
      "Epoch 8/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.6625 - acc: 0.8097 - val_loss: 0.7727 - val_acc: 0.8003\n",
      "Epoch 9/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.6564 - acc: 0.8106 - val_loss: 0.7796 - val_acc: 0.8010\n",
      "Epoch 10/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.6523 - acc: 0.8112 - val_loss: 0.7828 - val_acc: 0.8011\n",
      "Epoch 11/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.6496 - acc: 0.8115 - val_loss: 0.7738 - val_acc: 0.8021\n",
      "Epoch 12/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.6468 - acc: 0.8118 - val_loss: 0.7712 - val_acc: 0.8033\n",
      "Epoch 13/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.6435 - acc: 0.8130 - val_loss: 0.7562 - val_acc: 0.8038\n",
      "Epoch 14/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.6397 - acc: 0.8147 - val_loss: 0.7359 - val_acc: 0.8038\n",
      "Epoch 15/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.6364 - acc: 0.8179 - val_loss: 0.7469 - val_acc: 0.8055\n",
      "Epoch 16/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.6311 - acc: 0.8174 - val_loss: 0.7298 - val_acc: 0.8117\n",
      "Epoch 17/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.6245 - acc: 0.8218 - val_loss: 0.7210 - val_acc: 0.8119\n",
      "Epoch 18/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.6162 - acc: 0.8286 - val_loss: 0.6733 - val_acc: 0.8246\n",
      "Epoch 19/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.6037 - acc: 0.8376 - val_loss: 0.6712 - val_acc: 0.8318\n",
      "Epoch 20/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.5867 - acc: 0.8448 - val_loss: 0.6425 - val_acc: 0.8299\n",
      "Epoch 21/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.5666 - acc: 0.8538 - val_loss: 0.6575 - val_acc: 0.8449\n",
      "Epoch 22/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.5441 - acc: 0.8657 - val_loss: 0.6135 - val_acc: 0.8570\n",
      "Epoch 23/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.5196 - acc: 0.8730 - val_loss: 0.5709 - val_acc: 0.8614\n",
      "Epoch 24/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.4943 - acc: 0.8777 - val_loss: 0.5338 - val_acc: 0.8681\n",
      "Epoch 25/40\n",
      "1267/1267 [==============================] - 22s 17ms/step - loss: 0.4652 - acc: 0.8834 - val_loss: 0.5124 - val_acc: 0.8731\n",
      "Epoch 26/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.4318 - acc: 0.8911 - val_loss: 0.4850 - val_acc: 0.8747\n",
      "Epoch 27/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.3966 - acc: 0.8985 - val_loss: 0.4518 - val_acc: 0.8843\n",
      "Epoch 28/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.3615 - acc: 0.9086 - val_loss: 0.4224 - val_acc: 0.8969\n",
      "Epoch 29/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.3307 - acc: 0.9225 - val_loss: 0.3805 - val_acc: 0.9073\n",
      "Epoch 30/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.2952 - acc: 0.9327 - val_loss: 0.3505 - val_acc: 0.9172\n",
      "Epoch 31/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.2611 - acc: 0.9427 - val_loss: 0.3339 - val_acc: 0.9215\n",
      "Epoch 32/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.2319 - acc: 0.9496 - val_loss: 0.3151 - val_acc: 0.9255\n",
      "Epoch 33/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.2075 - acc: 0.9541 - val_loss: 0.3049 - val_acc: 0.9252\n",
      "Epoch 34/40\n",
      "1267/1267 [==============================] - 16s 13ms/step - loss: 0.1869 - acc: 0.9580 - val_loss: 0.2958 - val_acc: 0.9285\n",
      "Epoch 35/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.1696 - acc: 0.9615 - val_loss: 0.2891 - val_acc: 0.9295\n",
      "Epoch 36/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.1544 - acc: 0.9647 - val_loss: 0.2831 - val_acc: 0.9324\n",
      "Epoch 37/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.1413 - acc: 0.9679 - val_loss: 0.2755 - val_acc: 0.9347\n",
      "Epoch 38/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.1295 - acc: 0.9709 - val_loss: 0.2760 - val_acc: 0.9359\n",
      "Epoch 39/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.1193 - acc: 0.9730 - val_loss: 0.2695 - val_acc: 0.9376\n",
      "Epoch 40/40\n",
      "1267/1267 [==============================] - 16s 12ms/step - loss: 0.1110 - acc: 0.9752 - val_loss: 0.2616 - val_acc: 0.9399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdff3f42690>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_sentences_X, to_categorical(train_tags_y, len(tag2index)), batch_size=128, epochs=40, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "397/397 [==============================] - 2s 4ms/step\n",
      "(set(['acc']), set([93.87882302329884]))\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(test_sentences_X, to_categorical(test_tags_y, len(tag2index)))\n",
    "print ({model.metrics_names[1]},  {scores[1] * 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert back the categorical to tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_tokens(sequences, index):\n",
    "    token_sequences = []\n",
    "    for categorical_sequence in sequences:\n",
    "        token_sequence = []\n",
    "        for categorical in categorical_sequence:\n",
    "            token_sequence.append(index[np.argmax(categorical)])\n",
    " \n",
    "        token_sequences.append(token_sequence)\n",
    " \n",
    "    return token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n",
      "93\n",
      "397 397\n",
      "['N_NN', 'CC', 'PR_PRP', 'N_NN', 'N_NN', 'PSP', 'N_NN', 'RD_PUNC', 'V_VAUX', 'RB_AMN', 'RB_AMN', 'PR_PRP', 'V_VM', 'JJ', 'DT', 'PR_PRP', 'V_VM', 'CC', 'V_VM', 'PR_PRL', 'CC', 'V_VM', 'PR_PRL', 'PR_PRP', 'N_NN']\n",
      "['@' 'CC' 'PR_PRP' 'N_NN' 'N_NN' 'PSP' 'V_VM' 'RD_PUNC' 'V_VAUX' 'CC'\n",
      " 'V_VM' 'PR_PRP' 'V_VM' 'V_VM' 'PR_PRP' 'JJ' 'N_NN' 'CC' 'V_VM' 'N_NNP'\n",
      " 'CC' 'V_VM' 'N_NNP' 'RD_PUNC' 'N_NN']\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_sentences_X)\n",
    "\n",
    "print len(predictions[0])\n",
    "print len(test_tags_y[0])\n",
    "\n",
    "# print first few predictions\n",
    "token_sequences = logits_to_tokens(predictions, {i: t for t, i in tag2index.items()})\n",
    "# print(logits_to_tokens(predictions[:2], {i: t for t, i in tag2index.items()}))\n",
    "test_tags_sequences = logits_to_tokens(test_tags_y, {i: t for t, i in tag2index.items()})\n",
    "\n",
    "print len(predictions), len(test_tags_sequences)\n",
    "token_sequences[0] = filter(lambda a: a != '-PAD-', token_sequences[0])\n",
    "print token_sequences[0]\n",
    "print test_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
